{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook I will test how to implement a function that will take a csv and create a partitioned parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing partition locally to see if it works with pyspark\n",
    "\n",
    "cleaning my csv file with Csv class but changing the class so it doesnt convert to parquet. This is because its better to write the partitions directly to a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvCleaner:\n",
    "    @staticmethod\n",
    "    def timestamp_clean(df, col_name):\n",
    "        # convert the column to numeric with any errors(for example strings or letter) to NaN\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors=\"coerce\")\n",
    "        \n",
    "        df.dropna(subset=[col_name], inplace=True)\n",
    "\n",
    "        # Calculate the initial number of rows\n",
    "        initial_rows = df.shape[0]\n",
    "        \n",
    "        #Filter out rows with \"Timestamp\" values not containing 10 digits\n",
    "        df = df[df[col_name].apply(lambda x: len(str(int(x))) == 10)]\n",
    "\n",
    "        #calculate how many rows removed\n",
    "        rows_removed = initial_rows - df.shape[0]\n",
    "\n",
    "        # Convert the Unix timestamp to datetime with seconds\n",
    "        df[col_name] = pd.to_datetime(df[col_name], unit=\"s\")\n",
    "\n",
    "        # Sort the DataFrame by the timestamp column\n",
    "        df = df.sort_values(by=col_name)\n",
    "\n",
    "        return df, rows_removed\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_columns(df, col_name, low, high):\n",
    "    \n",
    "        # Convert column to numeric, making errors to Nan instead\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors=\"coerce\")\n",
    "\n",
    "        # Calculate the initial number of rows\n",
    "        initial_rows = df.shape[0]\n",
    "\n",
    "        df.loc[~df[col_name].between(low, high), col_name] = float('nan')\n",
    "\n",
    "        # Calculate the number of rows removed\n",
    "        rows_removed = initial_rows - df.shape[0]\n",
    "\n",
    "        #df[col_name] = df[col_name].interpolate()\n",
    "\n",
    "        return df, rows_removed\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_file(csv_file):\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "        total_rows_removed = 0\n",
    "\n",
    "        # Clean the DataFrame\n",
    "        for col in df.columns:\n",
    "            if \"Timestamp\" in col:\n",
    "                df, rows_removed = CsvCleaner.timestamp_clean(df, col)\n",
    "                total_rows_removed += rows_removed\n",
    "            \n",
    "            if \"speed_over_ground\" in col:\n",
    "                low = 0\n",
    "                high = 100\n",
    "                df, rows_removed = CsvCleaner.clean_columns(df, col, low, high)\n",
    "                total_rows_removed += rows_removed\n",
    "            \n",
    "            if \"Longitude\" in col:\n",
    "                low = -180\n",
    "                high = 180\n",
    "                df, rows_removed = CsvCleaner.clean_columns(df, col, low, high)\n",
    "                total_rows_removed += rows_removed\n",
    "\n",
    "            if \"Latitude\" in col:\n",
    "                low = -90\n",
    "                high = 90\n",
    "                df, rows_removed = CsvCleaner.clean_columns(df, col, low, high)\n",
    "                total_rows_removed += rows_removed\n",
    "\n",
    "            if \"engine_fuel_rate\" in col:\n",
    "                low = 0\n",
    "                high = 100\n",
    "                df,rows_removed = CsvCleaner.clean_columns(df, col, low, high)\n",
    "                total_rows_removed += rows_removed\n",
    "\n",
    "        output_dir = os.path.dirname(csv_file)\n",
    "        file_name = os.path.basename(csv_file)\n",
    "        file_name_without_extension, extension = os.path.splitext(file_name)\n",
    "        cleaned_csv_file = os.path.join(output_dir, file_name_without_extension + \"_cleaned.csv\")\n",
    "\n",
    "        df.to_csv(cleaned_csv_file, index=False)\n",
    "\n",
    "        print(f\"Total rows removed: {total_rows_removed}\")\n",
    "\n",
    "        return cleaned_csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_1 = \"../src/Data/vessel1_dummy_boat_data.csv\"\n",
    "vessel_2 = \"../src/Data/vessel2_dummy_boat_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows removed: 5\n",
      "Total rows removed: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1319/2019830326.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col_name] = pd.to_datetime(df[col_name], unit=\"s\")\n",
      "/tmp/ipykernel_1319/2019830326.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col_name] = pd.to_datetime(df[col_name], unit=\"s\")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CsvCleaner\n",
    "cleaner = CsvCleaner()\n",
    "\n",
    "# Clean the CSV file\n",
    "vessel_1_csv_file_path = cleaner.clean_file(vessel_1)\n",
    "vessel_2_csv_file_path = cleaner.clean_file(vessel_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have cleaned the csv files I will now try and partition and create parquet files locally to see if the process works. Then I will try and connect the process with my personal AWS and with my lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SparkSession' from 'pyspark' (/home/scglass/Github/LIA/parquet_conversion/venv/lib/python3.11/site-packages/pyspark/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_file_name, year, month, dayofmonth, regexp_extract\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SparkSession' from 'pyspark' (/home/scglass/Github/LIA/parquet_conversion/venv/lib/python3.11/site-packages/pyspark/__init__.py)"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, year, month, dayofmonth, regexp_extract\n",
    "\n",
    "# Initialize a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    app.Name(\"LocalParquetConversion\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to local sample dataset\n",
    "source_path = \"file:///../src/Data/cleaned_csv_files\"\n",
    "\n",
    "# Read data from local files\n",
    "source_df = spark.read.csv(source_path, header=True)\n",
    "\n",
    "# Extract vessel name from file name\n",
    "source_df = source_df.withColumn(\"file_name\", input_file_name())\n",
    "source_df = source_df.withColumn(\"vessel\", regexp_extract(\"filename\", \"([^\\/]+)(?=\\.[\\w]+$)\", 1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
